{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dac76b-dce3-4572-9e7d-ad86aed11124",
   "metadata": {},
   "source": [
    "**Student ID:** Sumeet Kumar - s222615086 | Akash Verma - s223669999 | Ho-se Kim - s224564517\n",
    "\n",
    "**Student Name:** Sumeet Kumar | Akash Verma | Ho-se Kim\n",
    "\n",
    "**Workshop / Lab Session Time:** Tuesday - 11:00 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b81c5f2-3e57-42a6-9901-c73b98492b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, dayofweek, countDistinct, month\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfff219-feef-4259-a2e0-570ec4580806",
   "metadata": {
    "id": "4dfff219-feef-4259-a2e0-570ec4580806"
   },
   "source": [
    "Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48590ee4-aabe-45b0-b203-371902eeb577",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48590ee4-aabe-45b0-b203-371902eeb577",
    "outputId": "9899debc-a047-4f7c-f512-0237835f3e29"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/hosek/OneDrive - Deakin University/Deakin 2024 Sem 2/SIT742 Modern Data Science/Assessments/Assessment 2/Modern-Data-Science-Repo/Modern-Data-Science/transactionrecord.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.repl.eagerEval.truncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the transaction data from the CSV file\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m transaction \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransactionrecord.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1.1.1 Replace 'NA' values with '-1' in the CustomerNo column as part of data cleaning\u001b[39;00m\n\u001b[0;32m     12\u001b[0m transaction \u001b[38;5;241m=\u001b[39m transaction\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, regexp_replace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/hosek/OneDrive - Deakin University/Deakin 2024 Sem 2/SIT742 Modern Data Science/Assessments/Assessment 2/Modern-Data-Science-Repo/Modern-Data-Science/transactionrecord.csv."
     ]
    }
   ],
   "source": [
    "# Start or get the existing Spark session\n",
    "spark = SparkSession.builder.appName(\"DataWrangling\").getOrCreate()\n",
    "\n",
    "# Configure Spark session to show more characters in each column and prevent output truncation\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "\n",
    "# Load the transaction data from the CSV file\n",
    "transaction = spark.read.csv('transactionrecord.csv', header=True, inferSchema=True)\n",
    "\n",
    "# 1.1.1 Replace 'NA' values with '-1' in the CustomerNo column as part of data cleaning\n",
    "transaction = transaction.withColumn(\"CustomerNo\", regexp_replace(\"CustomerNo\", \"NA\", \"-1\"))\n",
    "\n",
    "# 1.1.2 Remove non-alphabet characters from the productName column and store the cleaned names in a new column called productName_process\n",
    "transaction = transaction.withColumn(\"productName_process\", regexp_replace(\"productName\", \"[^a-zA-Z ]\", \"\"))\n",
    "\n",
    "# 1.1.2 Display the first 5 rows of the DataFrame to verify the modifications\n",
    "transaction.select(\"CustomerNo\", \"productName\", \"productName_process\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb1c49-a285-4d42-b604-1960e1b7705e",
   "metadata": {
    "id": "29fb1c49-a285-4d42-b604-1960e1b7705e"
   },
   "source": [
    "Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94003b4-3b1b-48ba-9dbc-4c0bdf27834e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "f94003b4-3b1b-48ba-9dbc-4c0bdf27834e",
    "outputId": "463b04e5-e99b-496c-c817-d0a7d1e1217a"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# 1.2.1 Calculate revenue by multiplying 'price' and 'Quantity', ensuring the result is stored as a float\n",
    "transaction = transaction.withColumn(\"Revenue\", (col(\"price\") * col(\"Quantity\")).cast(\"float\"))\n",
    "\n",
    "# 1.2.2 Convert the Spark DataFrame to a Pandas DataFrame for advanced processing and rename relevant columns\n",
    "df = transaction.toPandas()\n",
    "df.rename(columns={'Revenue': 'revenue', 'Date': 'transaction_date'}, inplace=True)\n",
    "\n",
    "# Convert the 'transaction_date' column to datetime format for accurate time series analysis\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "# 1.2.3 Plot the total revenue over time to identify trends and insights\n",
    "plt.figure(figsize=(10, 5))\n",
    "df.groupby('transaction_date')['revenue'].sum().plot(kind='line', title='Revenue Over Time')\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4fcdf-31f0-44f8-9587-42740d090c69",
   "metadata": {
    "id": "d0c4fcdf-31f0-44f8-9587-42740d090c69"
   },
   "source": [
    "The revenue shows significant volatility over time with noticeable peaks and troughs. This could indicate variability in sales due to factors like seasonal demand, promotions, or changes in consumer behavior\n",
    "\n",
    "The sharp peaks suggest that there are specific days or periods with exceptionally high sales. Identifying these can help in planning better inventory and marketing strategies.\n",
    "\n",
    "The valleys, especially those that dip significantly, might indicate periods of low activity where consumer engagement could be improved. Analyzing what contributes to these dips—be it inventory issues, lesser marketing efforts, or external economic factors—could provide actionable insights.\n",
    "\n",
    "A moving average or trend line overlaid on this plot could help in understanding the underlying trend beyond the weekly or monthly fluctuations. This can be particularly useful for strategic planning and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe9ccc-d1e6-47ef-84f4-5bb9139dc1c1",
   "metadata": {
    "id": "4abe9ccc-d1e6-47ef-84f4-5bb9139dc1c1"
   },
   "source": [
    "Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f099f4-7c4f-4c06-a13c-cf09a882b108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47f099f4-7c4f-4c06-a13c-cf09a882b108",
    "outputId": "eb60bce6-f2e5-48f1-eaa9-5dddbee50a59"
   },
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is cast to date type to facilitate extraction of the day of the week\n",
    "transaction = transaction.withColumn(\"Date\", transaction[\"Date\"].cast(\"date\"))\n",
    "\n",
    "# Add a 'DayOfWeek' column representing the day of the week as an integer (Sunday=1, Saturday=7)\n",
    "transaction = transaction.withColumn(\"DayOfWeek\", dayofweek(transaction[\"Date\"]))\n",
    "\n",
    "# 1.3.1 Calculate the average revenue per day of the week to identify which day typically sees the highest sales\n",
    "avg_revenue_by_day = transaction.groupBy(\"DayOfWeek\").avg(\"Revenue\").orderBy(\"avg(Revenue)\", ascending=False)\n",
    "avg_revenue_by_day.show(1)  # Display the day with the highest average revenue\n",
    "\n",
    "# Capture the day of the week with the highest average revenue for further analysis\n",
    "that_workday = avg_revenue_by_day.first()[\"DayOfWeek\"]\n",
    "\n",
    "# 1.3.2 Identify the top-selling product by revenue on 'that_workday'\n",
    "top_products_revenue = transaction.filter(transaction[\"DayOfWeek\"] == that_workday).groupBy(\"productName_process\").sum(\"Revenue\").orderBy(\"sum(Revenue)\", ascending=False)\n",
    "top_products_revenue.show(1)  # Display the top revenue product on 'that_workday'\n",
    "\n",
    "# Identify the top-selling product by quantity on 'that_workday'\n",
    "top_products_quantity = transaction.filter(transaction[\"DayOfWeek\"] == that_workday).groupBy(\"productName_process\").sum(\"Quantity\").orderBy(\"sum(Quantity)\", ascending=False)\n",
    "top_products_quantity.show(1)  # Display the top product by quantity on 'that_workday'\n",
    "\n",
    "# 1.3.3 Display the top 5 products by overall revenue across all days\n",
    "top_5_revenue_all_time = transaction.groupBy(\"productName_process\").sum(\"Revenue\").orderBy(\"sum(Revenue)\", ascending=False)\n",
    "top_5_revenue_all_time.show(5)  # Display the top 5 products by total revenue\n",
    "\n",
    "# Display the top 5 products by overall quantity across all days\n",
    "top_5_quantity_all_time = transaction.groupBy(\"productName_process\").sum(\"Quantity\").orderBy(\"sum(Quantity)\", ascending=False)\n",
    "top_5_quantity_all_time.show(5)  # Display the top 5 products by total quantity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd381b31-5a95-4cbd-a766-75c90bd233f6",
   "metadata": {
    "id": "bd381b31-5a95-4cbd-a766-75c90bd233f6"
   },
   "source": [
    "Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e3102-8e24-4fe1-bd24-a11741843e9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be5e3102-8e24-4fe1-bd24-a11741843e9f",
    "outputId": "9927a57a-a683-46c3-b807-4d061696a0bd"
   },
   "outputs": [],
   "source": [
    "# 1.4.1 Aggregate revenue by country to identify which country generates the highest total revenue\n",
    "revenue_by_country = transaction.groupBy(\"Country\").sum(\"Revenue\").withColumnRenamed(\"sum(Revenue)\", \"TotalRevenue\")\n",
    "\n",
    "# Identify the country with the highest revenue\n",
    "top_country = revenue_by_country.orderBy(\"TotalRevenue\", ascending=False).first()\n",
    "\n",
    "# Display the country with the highest revenue and the total amount\n",
    "print(f\"Country with the highest revenue: {top_country['Country']} with total revenue: {top_country['TotalRevenue']}\")\n",
    "\n",
    "# 1.4.2 Filter the transaction data for the country with the highest revenue and aggregate revenue by month\n",
    "transaction_top_country = transaction.filter(transaction[\"Country\"] == top_country[\"Country\"])\n",
    "revenue_by_month = transaction_top_country.groupBy(month(\"Date\").alias(\"Month\")).sum(\"Revenue\").withColumnRenamed(\"sum(Revenue)\", \"MonthlyRevenue\")\n",
    "\n",
    "# Identify the month with the highest revenue within the top country\n",
    "top_month = revenue_by_month.orderBy(\"MonthlyRevenue\", ascending=False).first()\n",
    "\n",
    "# Display the month with the highest revenue within the top country\n",
    "print(f\"In {top_country['Country']}, the month with the highest revenue is: Month {top_month['Month']} with revenue: {top_month['MonthlyRevenue']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45ece6-a793-40d3-b1bc-cb6c82aa8839",
   "metadata": {
    "id": "ff45ece6-a793-40d3-b1bc-cb6c82aa8839"
   },
   "source": [
    "Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbe1ab-0766-446a-9f27-ba3804477062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bcbe1ab-0766-446a-9f27-ba3804477062",
    "outputId": "f3b540ec-2d25-46cc-f90d-b27451a4365e"
   },
   "outputs": [],
   "source": [
    "# 1.5.1 Determine the most frequent shopper by counting unique transaction numbers per customer\n",
    "customer_frequency = transaction.groupBy(\"CustomerNo\").agg(countDistinct(\"TransactionNo\").alias(\"ShoppingTrips\"))\n",
    "\n",
    "# Identify the customer with the highest number of distinct shopping trips\n",
    "most_frequent_customer = customer_frequency.orderBy(\"ShoppingTrips\", ascending=False).first()\n",
    "\n",
    "# Display the customer number and the number of shopping trips for the most frequent shopper\n",
    "print(f\"Most frequent shopper: CustomerNo {most_frequent_customer['CustomerNo']} with {most_frequent_customer['ShoppingTrips']} shopping trips.\")\n",
    "\n",
    "# 1.5.2 Analyze the buying preferences of the most frequent shopper by examining the products they purchase most\n",
    "# Filter transactions to include only those with positive quantity (indicating a purchase was made)\n",
    "product_preferences = transaction.filter((transaction[\"CustomerNo\"] == most_frequent_customer[\"CustomerNo\"]) & (transaction[\"Quantity\"] > 0))\n",
    "\n",
    "# Group by product name and sum the quantities to identify top products by purchase volume\n",
    "top_products = product_preferences.groupBy(\"productName_process\").sum(\"Quantity\").orderBy(\"sum(Quantity)\", ascending=False)\n",
    "\n",
    "# Display the top 5 products purchased by the most frequent shopper\n",
    "print(\"Top products purchased by the most frequent shopper:\")\n",
    "top_products.show(5)  # Shows the top 5 most purchased products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sR2l4gbK_oY0",
   "metadata": {
    "id": "sR2l4gbK_oY0"
   },
   "source": [
    "Question 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sEslFj7c_nTP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEslFj7c_nTP",
    "outputId": "94c4d0fc-356a-4eea-8812-f8ea03ab1a3d"
   },
   "outputs": [],
   "source": [
    "# 1.6.1 Filter transactions to include only those with a positive quantity (valid purchases)\n",
    "df_filtered = df[df['Quantity'] > 0]\n",
    "\n",
    "# Group by TransactionNo and aggregate product categories and product names into lists\n",
    "df_agg = df_filtered.groupby('TransactionNo').agg({\n",
    "    'Product_category': lambda x: list(x),\n",
    "    'productName_process': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Display the first 5 rows to verify the aggregation\n",
    "print(df_agg.head())\n",
    "\n",
    "# 1.6.2 Define a function to remove adjacent duplicates from a list\n",
    "def remove_adjacent_duplicates(lst):\n",
    "    return [v for i, v in enumerate(lst) if i == 0 or v != lst[i - 1]]\n",
    "\n",
    "# Apply the function to remove adjacent duplicates in the Product_category column\n",
    "df_agg['Product_category'] = df_agg['Product_category'].apply(remove_adjacent_duplicates)\n",
    "\n",
    "# Save the processed DataFrame as 'df_1' and display the top 10 rows for verification\n",
    "df_1 = df_agg.copy()\n",
    "print(df_1.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u134HstA_s0t",
   "metadata": {
    "id": "u134HstA_s0t"
   },
   "source": [
    "Question 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_NUBu1z3_4oO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NUBu1z3_4oO",
    "outputId": "2b1c515b-1205-473d-cbdf-743047d559b1"
   },
   "outputs": [],
   "source": [
    "# 1.7.1 Add a new column 'prod_len' to df_1 that represents the length of the Product_category list for each transaction\n",
    "df_1['prod_len'] = df_1['Product_category'].apply(len)\n",
    "\n",
    "# Display the first 5 rows of df_1 to verify the addition of the 'prod_len' column\n",
    "print(df_1.head(5))\n",
    "\n",
    "# 1.7.2 Define a function to process the data by creating a 'path' column and filtering based on product category list length\n",
    "def data_processing(df, maxlength=3, minlength=1):\n",
    "    # Create a 'path' column by joining the product categories with 'start' and 'conversion' as the endpoints\n",
    "    df['path'] = df['Product_category'].apply(lambda x: ' > '.join(['start'] + x + ['conversion']))\n",
    "    \n",
    "    # Filter the DataFrame based on the length of the product category list, keeping rows within the specified range\n",
    "    df_2 = df[(df['prod_len'] <= maxlength) & (df['prod_len'] >= minlength)]\n",
    "    \n",
    "    return df_2\n",
    "\n",
    "# Apply the data_processing function to df_1 with maxlength=5 and minlength=2\n",
    "df_2 = data_processing(df_1, maxlength=5, minlength=2)\n",
    "\n",
    "# Display the top 10 rows of df_2 to verify the results\n",
    "print(df_2.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw_ThMut_49e",
   "metadata": {
    "id": "dw_ThMut_49e"
   },
   "source": [
    "Question 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819_8YkU_5TO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "819_8YkU_5TO",
    "outputId": "d5ab8551-a0b4-4f8a-8658-653721a8997a"
   },
   "outputs": [],
   "source": [
    "# 1.8.1 - 1.8.3 Count occurrences of specific patterns in the 'path' column\n",
    "\n",
    "# Count the number of transactions that end with the pattern ' > 0ca > conversion'\n",
    "pattern_ends = df_2['path'].str.endswith(' > 0ca > conversion').sum()\n",
    "\n",
    "# Count occurrences of the pattern '0ca > 1ca' within the 'path' column across all transactions\n",
    "pattern_contains = df_2['path'].str.count('0ca > 1ca').sum()\n",
    "\n",
    "# Print the results for the specific patterns\n",
    "print(f\"Transactions ending with ' > 0ca > conversion': {pattern_ends}\")\n",
    "print(f\"Occurrences of '0ca > 1ca' in the dataset: {pattern_contains}\")\n",
    "\n",
    "# 1.8.4 Calculate the ratio for the occurrences of specific patterns\n",
    "\n",
    "# Count the total occurrences of the pattern ' > 0ca > ' within the 'path' column\n",
    "total_occurrences = df_2['path'].str.count(' > 0ca > ').sum()\n",
    "\n",
    "# Calculate the ratio by dividing the pattern occurrences by the total occurrences\n",
    "ratio_sum = pattern_contains / total_occurrences\n",
    "\n",
    "# Print the final calculated ratio sum\n",
    "print(f\"Final ratio sum: {ratio_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "znzCfu-l_5wW",
   "metadata": {
    "id": "znzCfu-l_5wW"
   },
   "source": [
    "Question 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06V_GbD_6Im",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b06V_GbD_6Im",
    "outputId": "286e4d06-49e0-4710-db5b-5c98f64bc0b4"
   },
   "outputs": [],
   "source": [
    "# Task 1.9.1: Filter and pivot the dataframe to create a transaction-level product dataframe\n",
    "top_100_products = df.groupby('productName_process')['Quantity'].sum().nlargest(100).index\n",
    "df_filtered = df[df['productName_process'].isin(top_100_products) & (df['Quantity'] > 0)]\n",
    "transaction_df = df_filtered.pivot_table(index='TransactionNo', columns='productName_process', values='Quantity', fill_value=0)\n",
    "print(\"Transaction-Level Product DataFrame:\")\n",
    "print(transaction_df.head(5))\n",
    "\n",
    "# Task 1.9.2 - 1.9.4: Apply the Apriori algorithm and generate association rules\n",
    "\n",
    "# Convert the DataFrame to boolean (True/False) using the recommended approach\n",
    "transaction_df_bool = transaction_df > 0\n",
    "frequent_itemsets = apriori(transaction_df_bool.astype('bool'), min_support=0.015, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "print(\"\\nAssociation Rules with Minimum Support of 1.5% and Lift > 1.0:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Task 1.9.3: Apply the Apriori algorithm to identify items with support >= 1.0% and lift > 10\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets with min_support=1.0%\n",
    "frequent_itemsets_low_support = apriori(transaction_df_bool.astype('bool'), min_support=0.01, use_colnames=True)\n",
    "rules_high_lift = association_rules(frequent_itemsets_low_support, metric=\"lift\", min_threshold=10)\n",
    "print(\"\\nAssociation Rules with Support >= 1.0% and Lift > 10:\")\n",
    "print(rules_high_lift.head())\n",
    "\n",
    "# Task 1.9.4: Explore three more examples with different support, confidence, and lift measurements\n",
    "\n",
    "# Example 1: High Confidence\n",
    "rules_high_conf = association_rules(frequent_itemsets_low_support, metric=\"confidence\", min_threshold=0.8)\n",
    "print(\"\\nHigh Confidence Rules (Confidence >= 0.8):\")\n",
    "print(rules_high_conf.head())\n",
    "\n",
    "# Example 2: High Lift\n",
    "rules_very_high_lift = association_rules(frequent_itemsets_low_support, metric=\"lift\", min_threshold=5.0)\n",
    "print(\"\\nHigh Lift Rules (Lift >= 5.0):\")\n",
    "print(rules_very_high_lift.head())\n",
    "\n",
    "# Example 3: Lower Support\n",
    "rules_low_support = association_rules(frequent_itemsets_low_support, metric=\"lift\", min_threshold=1.0)\n",
    "print(\"\\nLow Support Rules (Support >= 1.0%):\")\n",
    "print(rules_low_support.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqgs5T3t_6pe",
   "metadata": {
    "id": "eqgs5T3t_6pe"
   },
   "source": [
    "Question 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h98EbTjJ_62m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h98EbTjJ_62m",
    "outputId": "93525f5f-4f86-4f30-c9aa-a14afc56341a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Task 1.10.1: Create the customer-product dataframe\n",
    "\n",
    "# Filter out transactions with negative quantities\n",
    "df_filtered = df[df['Quantity'] > 0]\n",
    "\n",
    "# Identify the top 100 products by quantity\n",
    "top_100_products = df_filtered.groupby('productName_process')['Quantity'].sum().nlargest(100).index\n",
    "\n",
    "# Filter the dataframe to keep only the top 100 products\n",
    "df_filtered = df_filtered[df_filtered['productName_process'].isin(top_100_products)]\n",
    "\n",
    "# Create the customer-product dataframe\n",
    "customer_product_df = df_filtered.pivot_table(index='CustomerNo',\n",
    "                                              columns='productName_process',\n",
    "                                              values='Quantity',\n",
    "                                              aggfunc='sum',\n",
    "                                              fill_value=0)\n",
    "\n",
    "# Display the first few rows to verify the DataFrame\n",
    "print(\"Customer-Product DataFrame:\")\n",
    "print(customer_product_df.head())\n",
    "\n",
    "# Task 1.10.2: Calculate pairwise Euclidean distance\n",
    "\n",
    "# Calculate pairwise Euclidean distance between customers\n",
    "distances = euclidean_distances(customer_product_df)\n",
    "\n",
    "# Convert the distance matrix to a DataFrame for easier interpretation\n",
    "distance_df = pd.DataFrame(distances, index=customer_product_df.index, columns=customer_product_df.index)\n",
    "\n",
    "# Display the distance DataFrame\n",
    "print(\"\\nPairwise Euclidean Distance between Customers:\")\n",
    "print(distance_df.head())\n",
    "\n",
    "# Task 1.10.3: Find the top 3 most similar customers\n",
    "\n",
    "def find_top_similar_customers(customer_no, distance_df):\n",
    "    if customer_no in distance_df.index:\n",
    "        customer_index = distance_df.index.get_loc(customer_no)\n",
    "        similar_customers = distance_df.iloc[customer_index].argsort()[1:4]  # Exclude the customer themselves\n",
    "        similar_customers_indices = distance_df.index[similar_customers]\n",
    "        return similar_customers_indices.tolist()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Find top 3 similar customers for CustomerNo 13069 and 17490\n",
    "similar_customers_13069 = find_top_similar_customers('13069', distance_df)\n",
    "similar_customers_17490 = find_top_similar_customers('17490', distance_df)\n",
    "\n",
    "print(f\"\\nTop 3 similar customers to 13069: {similar_customers_13069}\")\n",
    "print(f\"Top 3 similar customers to 17490: {similar_customers_17490}\")\n",
    "\n",
    "# Task 1.10.4: Product recommendations for CustomerNo 13069\n",
    "\n",
    "target_customer = '13069'\n",
    "\n",
    "if target_customer in customer_product_df.index:\n",
    "    customer_index = customer_product_df.index.get_loc(target_customer)\n",
    "\n",
    "    # Get the indices of the most similar customers\n",
    "    similar_customers = distance_df.iloc[customer_index].argsort()[1:4]  # Exclude the customer themselves\n",
    "    similar_customers_indices = customer_product_df.index[similar_customers]\n",
    "\n",
    "    # Products purchased by similar customers but not by target customer\n",
    "    target_customer_products = customer_product_df.loc[target_customer]\n",
    "    similar_customers_products = customer_product_df.loc[similar_customers_indices].mean()\n",
    "\n",
    "    # Filter to products the target customer hasn't purchased\n",
    "    recommendations = similar_customers_products[(similar_customers_products > 0) & (target_customer_products == 0)]\n",
    "    recommendations = recommendations.sort_values(ascending=False)\n",
    "\n",
    "    print(f\"\\nProduct recommendations for CustomerNo {target_customer}:\")\n",
    "    print(recommendations)\n",
    "else:\n",
    "    print(f\"CustomerNo {target_customer} does not exist in the DataFrame\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZbMZW0iv0vOZ",
   "metadata": {
    "id": "ZbMZW0iv0vOZ"
   },
   "source": [
    "# Part II Sales Prediction\n",
    "\n",
    "### Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4RhQ0VCi0tN6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "4RhQ0VCi0tN6",
    "outputId": "aee88217-65ac-48df-a043-55caa4b60dc5"
   },
   "outputs": [],
   "source": [
    "# Ensure the 'transaction_date' column is in datetime format\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "# Set 'transaction_date' as the index of the DataFrame\n",
    "df.set_index('transaction_date', inplace=True)\n",
    "\n",
    "# Calculate the mean revenue across all available data\n",
    "mean_revenue = df['revenue'].mean()\n",
    "\n",
    "# Resample the revenue data to a daily frequency, filling any missing dates with the calculated mean revenue\n",
    "daily_revenue = df['revenue'].resample('D').mean().fillna(mean_revenue)\n",
    "\n",
    "# Decompose the time series using an additive model to identify trend, seasonal, and residual components\n",
    "decomposition = seasonal_decompose(daily_revenue, model='additive')\n",
    "\n",
    "# Plot the decomposed components: observed, trend, seasonal, and residual\n",
    "plt.figure(figsize=(12, 8))\n",
    "decomposition.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ueAMfTXO0-aj",
   "metadata": {
    "id": "ueAMfTXO0-aj"
   },
   "source": [
    "### Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QdPgm9vt1CgT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QdPgm9vt1CgT",
    "outputId": "1df03617-c6b1-4391-ed8c-040ed3ac850c"
   },
   "outputs": [],
   "source": [
    "# Suppress specific warnings that might occur during ARIMA model fitting\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found.\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-stationary starting autoregressive parameters found.\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Maximum Likelihood optimization failed to converge.\")\n",
    "\n",
    "# Assume 'daily_revenue' has already been prepared from the previous task\n",
    "\n",
    "# Split the time series data into training and testing sets\n",
    "train_data = daily_revenue[:'2019-11-01']\n",
    "test_data = daily_revenue['2019-11-01':]\n",
    "\n",
    "# Initialize variables to store the best model, its parameters, and the lowest MAE\n",
    "best_mae = float('inf')  # Start with infinity as the benchmark for MAE\n",
    "best_order = None  # Placeholder for the best (p, d, q) combination\n",
    "best_model = None  # Placeholder for the best ARIMA model\n",
    "\n",
    "# Grid search across different combinations of (p, d, q) parameters\n",
    "for p in range(3):\n",
    "    for d in range(3):\n",
    "        for q in range(3):\n",
    "            try:\n",
    "                # Define and fit the ARIMA model with the current (p, d, q) parameters\n",
    "                model = ARIMA(train_data, order=(p, d, q))\n",
    "                model_fit = model.fit()\n",
    "                \n",
    "                # Forecast the test set using the fitted model\n",
    "                predictions = model_fit.forecast(steps=len(test_data))\n",
    "                \n",
    "                # Calculate the Mean Absolute Error (MAE) between the actual test set and the predictions\n",
    "                mae = mean_absolute_error(test_data, predictions)\n",
    "                \n",
    "                # Update the best model if the current MAE is lower than the best recorded MAE\n",
    "                if mae < best_mae:\n",
    "                    best_mae = mae\n",
    "                    best_order = (p, d, q)\n",
    "                    best_model = model_fit\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Print out the exception message for debugging purposes\n",
    "                print(f\"An error occurred for ARIMA({p},{d},{q}): {e}\")\n",
    "                continue\n",
    "\n",
    "# Output the best ARIMA model parameters and its corresponding MAE\n",
    "print(f\"Best ARIMA order: {best_order} with MAE: {best_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jZpTtUE1AMz",
   "metadata": {
    "id": "9jZpTtUE1AMz"
   },
   "source": [
    "### Question 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23a38d-8e2c-4cf2-a92f-5e74a379b35b",
   "metadata": {
    "id": "suaa2c4Z77X4"
   },
   "source": [
    "### Exploring Deep Learning Time Series Forecasting Methods\n",
    "\n",
    "Time series forecasting is a critical aspect of data science, and deep learning models have shown great promise in capturing complex patterns within time series data. Below, I will explore various deep learning methods suitable for time series forecasting, along with the necessary data wrangling and modeling steps. The discussion will cover Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Transformer models, and hybrid approaches like CNN-LSTM.\n",
    "\n",
    "### Deep Learning Methods for Time Series Forecasting\n",
    "\n",
    "1. **Recurrent Neural Networks (RNNs)**:\n",
    "   - **LSTM (Long Short-Term Memory)**: LSTMs are widely used for time series forecasting due to their ability to capture long-term dependencies in sequential data. They are particularly useful when the data exhibits temporal dependencies that span long periods.\n",
    "   - **GRU (Gated Recurrent Units)**: GRUs are a simplified version of LSTMs, offering similar performance but with fewer parameters, making them faster to train.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNNs)**:\n",
    "   - **Temporal Convolutional Networks (TCN)**: CNNs, and specifically TCNs, can be used to model time series data by capturing local temporal patterns through convolutional layers. They are effective when the data has short-term dependencies.\n",
    "\n",
    "3. **Transformer Models**:\n",
    "   - **Transformers**: Originally developed for natural language processing, transformers have been adapted for time series forecasting. They use self-attention mechanisms to capture long-range dependencies, making them suitable for complex time series data.\n",
    "\n",
    "4. **Hybrid Models**:\n",
    "   - **CNN-LSTM**: This hybrid model combines CNNs and LSTMs to capture both short-term and long-term dependencies in the data. CNN layers extract local features, while LSTM layers capture temporal dynamics.\n",
    "\n",
    "5. **Seq2Seq Models**:\n",
    "   - **Sequence-to-Sequence (Seq2Seq) Models**: These models are used for forecasting tasks where the input and output sequences have different lengths, such as multi-step forecasting.\n",
    "\n",
    "### Data Wrangling and Modeling Steps\n",
    "\n",
    "#### 1. Data Preparation\n",
    "\n",
    "**a. Data Collection**:\n",
    "   - Aggregate time series data at the appropriate frequency (e.g., daily, hourly). Ensure data is consistent and spans the necessary time periods.\n",
    "\n",
    "**b. Data Cleaning**:\n",
    "   - Handle missing values by imputing or interpolating, ensuring no gaps in the time series.\n",
    "   - Identify and remove outliers that may skew the model training.\n",
    "\n",
    "**c. Feature Engineering**:\n",
    "   - **Lag Features**: Create lagged versions of the target variable (e.g., revenue) to capture temporal dependencies.\n",
    "   - **Date/Time Features**: Extract features like day of the week, month, quarter, and holidays, which can help the model learn seasonal patterns.\n",
    "   - **Rolling/Aggregated Features**: Compute rolling means, sums, or standard deviations over various windows to capture trends and volatility.\n",
    "\n",
    "**d. Normalization/Standardization**:\n",
    "   - Normalize or standardize the data to ensure it is within a suitable range for the model. This is particularly important for models like LSTM and GRU.\n",
    "\n",
    "#### 2. Train-Test Split\n",
    "\n",
    "**a. Temporal Split**:\n",
    "   - Split the data into training and testing sets based on time, ensuring the model is only trained on past data and evaluated on future data.\n",
    "\n",
    "**b. Validation Split**:\n",
    "   - Further split the training set into training and validation subsets to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "#### 3. Model Selection and Configuration\n",
    "\n",
    "**a. Choose the Model**:\n",
    "   - Select a deep learning model based on the nature of the data:\n",
    "     - **LSTM/GRU**: For data with strong temporal dependencies.\n",
    "     - **CNN/TCN**: For data with clear local patterns.\n",
    "     - **Transformers**: For complex data with long-range dependencies.\n",
    "\n",
    "**b. Define the Architecture**:\n",
    "   - **Input Layer**: Define the input shape based on the number of lag features and other predictors.\n",
    "   - **Hidden Layers**: Add LSTM/GRU layers, CNN layers, or Transformer blocks depending on the model.\n",
    "   - **Dropout Layers**: Use dropout to prevent overfitting by randomly setting some layer outputs to zero during training.\n",
    "   - **Output Layer**: Configure the output layer based on whether the task is single-step or multi-step forecasting.\n",
    "\n",
    "**c. Compile the Model**:\n",
    "   - Choose an optimizer (e.g., Adam), loss function (e.g., Mean Squared Error), and evaluation metrics (e.g., MAE) that suit the forecasting task.\n",
    "\n",
    "**d. Train the Model**:\n",
    "   - Fit the model to the training data, using the validation set to monitor performance.\n",
    "   - Employ techniques like early stopping to prevent overfitting if the validation performance deteriorates.\n",
    "\n",
    "**e. Hyperparameter Tuning**:\n",
    "   - Use grid search or random search to find the best combination of hyperparameters, such as the number of layers, units per layer, learning rate, and dropout rate.\n",
    "\n",
    "#### 4. Model Evaluation and Interpretation\n",
    "\n",
    "**a. Evaluate the Model**:\n",
    "   - Test the model on the unseen test set using metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n",
    "   - Generate forecasts and compare them with actual values to assess accuracy.\n",
    "\n",
    "**b. Interpret Results**:\n",
    "   - Analyze the model’s residuals and error metrics to understand where the model performs well and where it struggles.\n",
    "   - Plot time series predictions against actual data to visually inspect forecast accuracy.\n",
    "\n",
    "**c. Compare with Baseline Models**:\n",
    "   - Compare the performance of the deep learning model with traditional models like ARIMA or Prophet to assess the added value of using deep learning.\n",
    "\n",
    "#### 5. Model Deployment\n",
    "\n",
    "**a. Export the Model**:\n",
    "   - Save the trained model in a format that can be deployed (e.g., HDF5 for Keras models).\n",
    "\n",
    "**b. Deploy the Model**:\n",
    "   - Deploy the model to a production environment where it can generate real-time forecasts.\n",
    "\n",
    "**c. Monitor the Model**:\n",
    "   - Continuously monitor the model’s performance over time. Retrain the model periodically with new data to ensure it remains accurate.\n",
    "\n",
    "### References for Deep Learning Time Series Forecasting Models\n",
    "\n",
    "1. **LSTM/GRU Networks**:\n",
    "   - Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation*, 9(8), 1735–1780.\n",
    "   - Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. *arXiv preprint arXiv:1406.1078*.\n",
    "\n",
    "2. **CNNs for Time Series**:\n",
    "   - Borovykh, A., Bohte, S., & Oosterlee, C. W. (2017). Conditional Time Series Forecasting with Convolutional Neural Networks. *arXiv preprint arXiv:1703.04691*.\n",
    "\n",
    "3. **Transformers**:\n",
    "   - Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In *Advances in Neural Information Processing Systems* (pp. 5998-6008).\n",
    "\n",
    "4. **Seq2Seq Models**:\n",
    "   - Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. *arXiv preprint arXiv:1409.3215*.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The exploration of deep learning methods for time series forecasting involves careful data preparation, model selection, and evaluation. By following the outlined steps, you can effectively apply models like LSTM, GRU, CNN, and Transformers to forecast time series data, thereby capturing complex patterns that traditional methods might miss. The provided references offer foundational insights into the models discussed."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
