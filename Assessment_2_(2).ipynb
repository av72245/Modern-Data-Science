{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dac76b-dce3-4572-9e7d-ad86aed11124",
   "metadata": {
    "id": "12dac76b-dce3-4572-9e7d-ad86aed11124"
   },
   "source": [
    "**Student ID:** Sumeet Kumar - s222615086 | Akash Verma - s223669999 | Ho-se Kim - s224564517\n",
    "\n",
    "**Student Name:** Sumeet Kumar | Akash Verma | Ho-se Kim\n",
    "\n",
    "**Workshop / Lab Session Time:** Tuesday - 11:00 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sa-IfgTFXrZq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sa-IfgTFXrZq",
    "outputId": "6e3caddd-2a03-4b88-fa8b-50213caba62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hosek\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hosek\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b81c5f2-3e57-42a6-9901-c73b98492b8e",
   "metadata": {
    "id": "9b81c5f2-3e57-42a6-9901-c73b98492b8e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col, dayofweek, avg, countDistinct, month, year, sum as _sum\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import gzip\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfff219-feef-4259-a2e0-570ec4580806",
   "metadata": {
    "id": "4dfff219-feef-4259-a2e0-570ec4580806"
   },
   "source": [
    "Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "IV7d1fxXpqkn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IV7d1fxXpqkn",
    "outputId": "8d2b21d9-36bd-4dc7-aa1c-622b11934b5c"
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/hosek/OneDrive - Deakin University/Deakin 2024 Sem 2/SIT742 Modern Data Science/Assessments/Assessment 2/Modern-Data-Science-Repo2/Modern-Data-Science-main/transactionrecord.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataWrangling\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the data from the unzipped CSV file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m transaction \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransactionrecord.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Task 1.1.1: Replace 'NA' with '-1' in CustomerNo\u001b[39;00m\n\u001b[0;32m      8\u001b[0m transaction \u001b[38;5;241m=\u001b[39m transaction\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, regexp_replace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerNo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/hosek/OneDrive - Deakin University/Deakin 2024 Sem 2/SIT742 Modern Data Science/Assessments/Assessment 2/Modern-Data-Science-Repo2/Modern-Data-Science-main/transactionrecord.csv."
     ]
    }
   ],
   "source": [
    "# Access the existing Spark session\n",
    "spark = SparkSession.builder.appName(\"DataWrangling\").getOrCreate()\n",
    "\n",
    "# Load the data from the unzipped CSV file\n",
    "transaction = spark.read.csv('transactionrecord.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Task 1.1.1: Replace 'NA' with '-1' in CustomerNo\n",
    "transaction = transaction.withColumn(\"CustomerNo\", regexp_replace(\"CustomerNo\", \"NA\", \"-1\"))\n",
    "\n",
    "# Task 1.1.2: Process the productName column, keep only alphabet characters\n",
    "transaction = transaction.withColumn(\"productName_process\", regexp_replace(\"productName\", \"[^a-zA-Z ]\", \"\"))\n",
    "\n",
    "# Show the first 5 rows of the processed dataframe\n",
    "transaction.show(5, truncate=False)\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above output displays the first 5 rows of the transaction data after replacing 'NA' with '-1' in CustomerNo and removing non-alphabetic characters from productName.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8MBxAKmaw7Gn",
   "metadata": {
    "id": "8MBxAKmaw7Gn"
   },
   "source": [
    "For Question 1.1, the CustomerNo column was processed to replace 'NA' with '-1,' and the productName column was cleaned by retaining only alphabetic characters. This preparation ensures that the dataset is ready for further analysis without missing or improperly formatted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb1c49-a285-4d42-b604-1960e1b7705e",
   "metadata": {
    "id": "29fb1c49-a285-4d42-b604-1960e1b7705e"
   },
   "source": [
    "Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pQdRCGOwpsBi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pQdRCGOwpsBi",
    "outputId": "c8655a5c-b2d3-4e7d-e27a-b9a8cea9abd9"
   },
   "outputs": [],
   "source": [
    "# Task 1.2.1: Calculate revenue and add it to the dataframe\n",
    "transaction = transaction.withColumn(\"revenue\", col(\"Price\") * col(\"Quantity\"))\n",
    "\n",
    "# Show the first 5 rows with the new revenue column\n",
    "transaction.show(5, truncate=False)\n",
    "\n",
    "# Task 1.2.2: Convert to pandas dataframe and create transaction_date column\n",
    "df = transaction.toPandas()\n",
    "df['transaction_date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Show the first 5 rows of the pandas dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above output shows the first 5 rows of the dataframe, including the calculated revenue and the new transaction_date column.\")\n",
    "\n",
    "# Task 1.2.3: Plot the sum of revenue on transaction_date\n",
    "# Group by transaction_date and sum the revenue\n",
    "daily_revenue = df.groupby('transaction_date')['revenue'].sum()\n",
    "\n",
    "# Plotting the sum of revenue by transaction_date\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_revenue.index, daily_revenue.values, linestyle='-', color='green')\n",
    "plt.title('Total Revenue by Transaction Date')\n",
    "plt.xlabel('Transaction Date')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above plot shows the total revenue over time, highlighting the daily revenue trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4fcdf-31f0-44f8-9587-42740d090c69",
   "metadata": {
    "id": "d0c4fcdf-31f0-44f8-9587-42740d090c69"
   },
   "source": [
    "The revenue shows significant volatility over time with noticeable peaks and troughs. This could indicate variability in sales due to factors like seasonal demand, promotions, or changes in consumer behavior\n",
    "\n",
    "The sharp peaks suggest that there are specific days or periods with exceptionally high sales. Identifying these can help in planning better inventory and marketing strategies.\n",
    "\n",
    "The valleys, especially those that dip significantly, might indicate periods of low activity where consumer engagement could be improved. Analyzing what contributes to these dips—be it inventory issues, lesser marketing efforts, or external economic factors—could provide actionable insights.\n",
    "\n",
    "A moving average or trend line overlaid on this plot could help in understanding the underlying trend beyond the weekly or monthly fluctuations. This can be particularly useful for strategic planning and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe9ccc-d1e6-47ef-84f4-5bb9139dc1c1",
   "metadata": {
    "id": "4abe9ccc-d1e6-47ef-84f4-5bb9139dc1c1"
   },
   "source": [
    "Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1v7cTW86puq6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1v7cTW86puq6",
    "outputId": "e0b61163-5d05-41ad-cc95-e864c6ce50fc"
   },
   "outputs": [],
   "source": [
    "# Ensure the transaction_date column is in datetime format\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'])\n",
    "\n",
    "# Convert the pandas dataframe back to a Spark dataframe for further processing\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Task 1.3.1: Calculate average revenue by workday\n",
    "spark_df = spark_df.withColumn('day_of_week', dayofweek(col('transaction_date')))\n",
    "\n",
    "# Calculate average revenue by day of the week\n",
    "avg_revenue_by_day = spark_df.groupBy('day_of_week').agg(avg('revenue').alias('avg_revenue'))\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "avg_revenue_by_day_pd = avg_revenue_by_day.toPandas()\n",
    "\n",
    "# Ensure all days of the week are represented\n",
    "all_days = pd.DataFrame({'day_of_week': range(1, 8)})\n",
    "avg_revenue_by_day_pd = all_days.merge(avg_revenue_by_day_pd, on='day_of_week', how='left').fillna(0)\n",
    "\n",
    "# Plot the average revenue by day of the week with connected lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_revenue_by_day_pd['day_of_week'], avg_revenue_by_day_pd['avg_revenue'], marker='o', linestyle='-', color='blue')\n",
    "plt.title('Average Revenue by Day of the Week')\n",
    "plt.xlabel('Day of the Week (1=Sunday, ..., 7=Saturday)')\n",
    "plt.ylabel('Average Revenue')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above plot shows the average revenue generated on each day of the week.\")\n",
    "\n",
    "# Task 1.3.2: Find the workday with the highest average revenue\n",
    "max_revenue_day = avg_revenue_by_day_pd.loc[avg_revenue_by_day_pd['avg_revenue'].idxmax(), 'day_of_week']\n",
    "\n",
    "# Filter data to only include transactions on the day with the highest revenue\n",
    "max_revenue_day_df = spark_df.filter(spark_df['day_of_week'] == max_revenue_day)\n",
    "\n",
    "# Calculate total revenue and sales volume by product on that day\n",
    "product_revenue = max_revenue_day_df.groupBy('productName_process').agg(\n",
    "    _sum('revenue').alias('total_revenue'),\n",
    "    _sum('Quantity').alias('total_quantity')\n",
    ")\n",
    "\n",
    "# Convert to pandas for easier manipulation\n",
    "product_revenue_pd = product_revenue.toPandas()\n",
    "\n",
    "# Identify the product with the highest revenue and the product with the highest sales volume\n",
    "highest_revenue_product = product_revenue_pd.loc[product_revenue_pd['total_revenue'].idxmax(), 'productName_process']\n",
    "highest_sales_volume_product = product_revenue_pd.loc[product_revenue_pd['total_quantity'].idxmax(), 'productName_process']\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest revenue product on workday {max_revenue_day}: {highest_revenue_product}\")\n",
    "print(f\"Highest sales volume product on workday {max_revenue_day}: {highest_sales_volume_product}\")\n",
    "\n",
    "# Task 1.3.3: Provide two plots showing the top 5 products by revenue and sales volume in general\n",
    "\n",
    "# Calculate total revenue and sales volume by product for the entire dataset\n",
    "overall_product_revenue = spark_df.groupBy('productName_process').agg(\n",
    "    _sum('revenue').alias('total_revenue'),\n",
    "    _sum('Quantity').alias('total_quantity')\n",
    ").toPandas()\n",
    "\n",
    "# Plot top 5 products by overall revenue\n",
    "top_5_overall_revenue = overall_product_revenue.nlargest(5, 'total_revenue')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_5_overall_revenue['productName_process'], top_5_overall_revenue['total_revenue'], color='green')\n",
    "plt.title('Top 5 Products by Overall Revenue')\n",
    "plt.xlabel('Total Revenue')\n",
    "plt.ylabel('Product Name')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot top 5 products by overall sales volume\n",
    "top_5_overall_volume = overall_product_revenue.nlargest(5, 'total_quantity')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_5_overall_volume['productName_process'], top_5_overall_volume['total_quantity'], color='orange')\n",
    "plt.title('Top 5 Products by Overall Sales Volume')\n",
    "plt.xlabel('Total Sales Volume')\n",
    "plt.ylabel('Product Name')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above plots show the top 5 products by overall revenue and sales volume.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIuJwEGMJ5i_",
   "metadata": {
    "id": "xIuJwEGMJ5i_"
   },
   "source": [
    "1.3.1 Average Revenue by Day of the Week\n",
    "\n",
    "The analysis reveals the workday with the highest average revenue, visualized in a line chart. This insight is crucial for optimizing business operations and scheduling marketing efforts on days with the highest consumer activity.\n",
    "\n",
    "1.3.2 Top-Selling Products on the Highest Revenue Day\n",
    "\n",
    "On the identified high-revenue day, the top-selling products are analyzed both by revenue and quantity. These products are key drivers of sales and should be prioritized in inventory management and promotional campaigns.\n",
    "\n",
    "1.3.3 Top 5 Products by Overall Revenue and Quantity\n",
    "\n",
    "Two bar charts display the top 5 products contributing the most revenue and those with the highest sales volumes. These products represent consistent consumer demand, and ensuring their availability could enhance overall sales performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd381b31-5a95-4cbd-a766-75c90bd233f6",
   "metadata": {
    "id": "bd381b31-5a95-4cbd-a766-75c90bd233f6"
   },
   "source": [
    "Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jv4mTbfLpwDG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "Jv4mTbfLpwDG",
    "outputId": "f186c795-d70d-461b-e23f-4fc521a8f1bb"
   },
   "outputs": [],
   "source": [
    "# Task 1.4: Identify the country with the highest revenue and the month in that country with the highest revenue\n",
    "\n",
    "# Calculate total revenue by country\n",
    "country_revenue = spark_df.groupBy('Country').agg(_sum('revenue').alias('total_revenue'))\n",
    "\n",
    "# Convert to pandas dataframe for easier manipulation\n",
    "country_revenue_pd = country_revenue.toPandas()\n",
    "\n",
    "# Identify the country with the highest total revenue\n",
    "highest_revenue_country = country_revenue_pd.loc[country_revenue_pd['total_revenue'].idxmax(), 'Country']\n",
    "print(f\"Country with the highest revenue: {highest_revenue_country}\")\n",
    "\n",
    "# Filter data for the country with the highest revenue\n",
    "highest_revenue_country_df = spark_df.filter(spark_df['Country'] == highest_revenue_country)\n",
    "\n",
    "# Extract month and year from the transaction date for monthly revenue calculation\n",
    "highest_revenue_country_df = highest_revenue_country_df.withColumn('month', month(col('transaction_date')))\n",
    "highest_revenue_country_df = highest_revenue_country_df.withColumn('year', year(col('transaction_date')))\n",
    "\n",
    "# Calculate total revenue by month within the highest revenue country\n",
    "monthly_revenue = highest_revenue_country_df.groupBy('year', 'month').agg(_sum('revenue').alias('total_revenue'))\n",
    "\n",
    "# Convert to pandas dataframe for easier manipulation\n",
    "monthly_revenue_pd = monthly_revenue.toPandas()\n",
    "\n",
    "# Identify the month with the highest revenue\n",
    "max_revenue_month = monthly_revenue_pd.loc[monthly_revenue_pd['total_revenue'].idxmax()]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Month with the highest revenue in {highest_revenue_country}: {max_revenue_month['year']}-{int(max_revenue_month['month']):02d} with a total revenue of {max_revenue_month['total_revenue']}\")\n",
    "\n",
    "# Plotting total revenue by month in the country with the highest revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_revenue_pd['year'].astype(str) + '-' + monthly_revenue_pd['month'].astype(str).str.zfill(2),\n",
    "         monthly_revenue_pd['total_revenue'], marker='o', linestyle='-', color='purple')\n",
    "plt.title(f\"Monthly Revenue in {highest_revenue_country}\")\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pc5APcvZxL6u",
   "metadata": {
    "id": "Pc5APcvZxL6u"
   },
   "source": [
    "For Question 1.4, the United Kingdom was identified as the country with the highest revenue. Further analysis revealed that November is the month with the highest revenue in this country. This insight can help focus marketing and inventory efforts during peak sales periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45ece6-a793-40d3-b1bc-cb6c82aa8839",
   "metadata": {
    "id": "ff45ece6-a793-40d3-b1bc-cb6c82aa8839"
   },
   "source": [
    "Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irG2DBgiqUvD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "irG2DBgiqUvD",
    "outputId": "eb50e5c9-55b9-4241-e71a-16e8ab4f334c"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Task 1.5.1: Determine the shopping frequency of customers (filter out non-shopping transactions)\n",
    "# Filter out transactions where Quantity <= 0\n",
    "valid_transactions = transaction.filter(transaction[\"Quantity\"] > 0)\n",
    "\n",
    "# Calculate the distinct count of transactionNo for each customer\n",
    "shopping_frequency = valid_transactions.groupBy(\"CustomerNo\").agg(countDistinct(\"TransactionNo\").alias(\"ShoppingFrequency\"))\n",
    "\n",
    "# Identify the customer who shops most frequently\n",
    "most_frequent_customer = shopping_frequency.orderBy(\"ShoppingFrequency\", ascending=False).first()\n",
    "\n",
    "# Display the customer who shops most frequently and their shopping frequency\n",
    "print(f\"Customer with the highest shopping frequency: {most_frequent_customer['CustomerNo']} with {most_frequent_customer['ShoppingFrequency']} distinct transactions\")\n",
    "\n",
    "# Task 1.5.2: Find out what products this customer typically buys based on the Quantity\n",
    "# Filter transactions for the most frequent customer\n",
    "customer_transactions = valid_transactions.filter(valid_transactions[\"CustomerNo\"] == most_frequent_customer[\"CustomerNo\"])\n",
    "\n",
    "# Aggregate the total Quantity of each product purchased by this customer\n",
    "customer_product_quantities = customer_transactions.groupBy(\"productName_process\").agg(_sum(\"Quantity\").alias(\"TotalQuantity\"))\n",
    "\n",
    "# Identify the products this customer typically buys (sorted by Quantity)\n",
    "top_products = customer_product_quantities.orderBy(\"TotalQuantity\", ascending=False)\n",
    "\n",
    "# Display the top products that the most frequent customer typically buys\n",
    "top_products.show(truncate=False)\n",
    "\n",
    "# Output explanation\n",
    "print(\"The above output lists the products most frequently purchased by the customer with the highest shopping frequency, sorted by the total quantity purchased.\\n\")\n",
    "\n",
    "# Optional: Plotting the top products by quantity purchased by the most frequent customer\n",
    "top_products_pd = top_products.toPandas()\n",
    "\n",
    "# Plot the top 5 products by quantity purchased\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_products_pd['productName_process'].head(5), top_products_pd['TotalQuantity'].head(5), color='blue')\n",
    "plt.title(f\"Top 5 Products Purchased by Customer {most_frequent_customer['CustomerNo']}\")\n",
    "plt.xlabel('Total Quantity Purchased')\n",
    "plt.ylabel('Product Name')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output explanation\n",
    "print(f\"The above plot shows the top 5 products purchased by the most frequent customer (CustomerNo: {most_frequent_customer['CustomerNo']}), based on the quantity purchased.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGlTlJZpxREQ",
   "metadata": {
    "id": "HGlTlJZpxREQ"
   },
   "source": [
    "For Question 1.5, customer transactions were analyzed to identify the customer with the highest shopping frequency. Additionally, the top products typically purchased by this customer were determined, which can inform personalized marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sR2l4gbK_oY0",
   "metadata": {
    "id": "sR2l4gbK_oY0"
   },
   "source": [
    "Question 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sEslFj7c_nTP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEslFj7c_nTP",
    "outputId": "724e6b71-52a2-4392-ca39-e5a084e512bb"
   },
   "outputs": [],
   "source": [
    "# Task 1.6.1: Filter the dataframe to include only rows where Quantity > 0\n",
    "df = df[df['Quantity'] > 0]\n",
    "\n",
    "# Group by transactionNo and aggregate product_category and productName_process into lists\n",
    "df_grouped = df.groupby('TransactionNo').agg({\n",
    "    'Product_category': lambda x: list(x),\n",
    "    'productName_process': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Display the first 10 rows of the grouped dataframe to verify the result\n",
    "print(df_grouped.head(10))\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the dataframe after filtering for positive quantities and grouping by TransactionNo to aggregate product categories and product names into lists.\\n\")\n",
    "\n",
    "# Task 1.6.2: Remove adjacent duplicates from the product_category lists\n",
    "\n",
    "# Define a function to remove adjacent duplicates from a list\n",
    "def remove_adjacent_duplicates(lst):\n",
    "    return [v for i, v in enumerate(lst) if i == 0 or v != lst[i-1]]\n",
    "\n",
    "# Apply the function to the product_category column\n",
    "df_grouped['Product_category'] = df_grouped['Product_category'].apply(remove_adjacent_duplicates)\n",
    "\n",
    "# Save the processed dataframe as 'df_1'\n",
    "df_1 = df_grouped\n",
    "\n",
    "# Display the first 10 rows of the processed dataframe to verify the changes\n",
    "print(df_1.head(10))\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the dataframe after removing adjacent duplicates from the product_category lists and saving the processed dataframe as 'df_1'.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u134HstA_s0t",
   "metadata": {
    "id": "u134HstA_s0t"
   },
   "source": [
    "Question 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_NUBu1z3_4oO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NUBu1z3_4oO",
    "outputId": "c0f0761a-041d-48b7-f2c9-0ce23cdec031"
   },
   "outputs": [],
   "source": [
    "# Task 1.7.1: Add a new column 'prod_len' that represents the length of the Product_category list for each transaction\n",
    "df_1['prod_len'] = df_1['Product_category'].apply(len)\n",
    "\n",
    "# Display the first 5 rows of df_1 to verify the addition of the 'prod_len' column\n",
    "print(df_1.head(5))\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the first 5 rows of the dataframe 'df_1' after adding the 'prod_len' column, which indicates the length of the Product_category list for each transaction.\\n\")\n",
    "\n",
    "# Task 1.7.2: Define a function to process the data by creating a 'path' column using str.replace() and filtering based on product category list length\n",
    "def data_processing(df, maxlength=5, minlength=2):\n",
    "    # Convert the Product_category list into a string and format it into a path string using str.replace()\n",
    "    df['path'] = df['Product_category'].apply(lambda x: 'start > ' + str(x).replace('[', '').replace(']', '').replace(\"'\", \"\").replace(', ', ' > ') + ' > conversion')\n",
    "\n",
    "    # Filter the DataFrame based on the length of the product category list, keeping rows within the specified range\n",
    "    df_2 = df[(df['prod_len'] <= maxlength) & (df['prod_len'] >= minlength)]\n",
    "\n",
    "    return df_2\n",
    "\n",
    "# Apply the data_processing function to df_1 with maxlength=5 and minlength=2\n",
    "df_2 = data_processing(df_1, maxlength=5, minlength=2)\n",
    "\n",
    "# Display the top 10 rows of df_2 to verify the results\n",
    "print(df_2.head(10))\n",
    "\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the top 10 rows of the dataframe 'df_2' after processing the product category lists into paths using str.replace() and filtering based on the list length.\\n\")\n",
    "\n",
    "# Visualization: Plotting the Top 10 Most Common Product Paths\n",
    "path_counts = Counter(df_2['path'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(*zip(*path_counts.most_common(10)))\n",
    "plt.title('Top 10 Most Common Product Paths')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Product Path')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cafe2-2ab6-401b-bfdd-7e99479d8781",
   "metadata": {
    "id": "612cafe2-2ab6-401b-bfdd-7e99479d8781"
   },
   "source": [
    "The bar plot visualizes the key output of question 1.7.2, showing the most common paths customers follow when purchasing products. This visualization ties directly back to the analysis objective of understanding customer behavior in terms of product category transitions during transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dw_ThMut_49e",
   "metadata": {
    "id": "dw_ThMut_49e"
   },
   "source": [
    "Question 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819_8YkU_5TO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "819_8YkU_5TO",
    "outputId": "1e6fb112-9068-4cd2-9610-36917b7bce68"
   },
   "outputs": [],
   "source": [
    "# Task 1.8: Analyze transaction patterns in df_2\n",
    "\n",
    "# Define the patterns to search for in 1.8.1 and 1.8.2\n",
    "end_patterns = ['0ca > conversion', '1ca > conversion', '2ca > conversion', '3ca > conversion', '4ca > conversion']\n",
    "substring_patterns = ['0ca > 0ca', '0ca > 1ca', '0ca > 2ca', '0ca > 3ca', '0ca > 4ca', '0ca > conversion']\n",
    "general_pattern = '0ca'\n",
    "\n",
    "# Initialize dictionaries to store the counts for end patterns and substring patterns\n",
    "end_counts = {pattern: 0 for pattern in end_patterns}\n",
    "substring_counts = {pattern: 0 for pattern in substring_patterns}\n",
    "\n",
    "# Initialize the general pattern count for 1.8.3\n",
    "general_count = 0\n",
    "\n",
    "# Loop through each transaction path in df_2 to calculate the counts\n",
    "for path in df_2['path']:\n",
    "    # Task 1.8.1: Count the number of transactions ending with specific patterns\n",
    "    for pattern in end_patterns:\n",
    "        if path.endswith(pattern):\n",
    "            end_counts[pattern] += 1\n",
    "\n",
    "    # Task 1.8.2: Count the occurrences of specific substring patterns in the transactions\n",
    "    for pattern in substring_patterns:\n",
    "        substring_counts[pattern] += path.count(pattern)\n",
    "\n",
    "    # Task 1.8.3: Count the total occurrences of the general pattern '... > 0ca > ...'\n",
    "    general_count += path.count(f'> {general_pattern} >')\n",
    "\n",
    "# Display the counts for Task 1.8.1\n",
    "print(\"Number of transactions ending with specific patterns:\")\n",
    "for pattern, count in end_counts.items():\n",
    "    print(f\"{pattern}: {count}\")\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the number of transactions in 'df_2' that end with each of the specified patterns.\\n------\\n\")\n",
    "\n",
    "# Display the counts for Task 1.8.2\n",
    "print(\"Number of occurrences of specific substrings in transactions:\")\n",
    "for pattern, count in substring_counts.items():\n",
    "    print(f\"{pattern}: {count}\")\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the total number of occurrences of each specified substring pattern in the transactions within 'df_2'.\\n------\\n\")\n",
    "\n",
    "# Display the result for Task 1.8.3\n",
    "print(f\"Total occurrences of the pattern '... > 0ca > ...' in transactions: {general_count}\")\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the total number of occurrences of the pattern '... > 0ca > ...' in the transactions within 'df_2'.\\n------\\n\")\n",
    "\n",
    "# Task 1.8.4: Compute the ratio of each substring occurrence from 1.8.2 to the general pattern count from 1.8.3 and sum them\n",
    "if general_count > 0:\n",
    "    ratios_sum = sum(substring_counts[pattern] / general_count for pattern in substring_patterns)\n",
    "else:\n",
    "    ratios_sum = 0  # To handle division by zero\n",
    "\n",
    "# Display the result for Task 1.8.4\n",
    "print(f\"Sum of the ratios of substring occurrences to the general pattern occurrence: {ratios_sum}\")\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the sum of the ratios of specific substring occurrences to the general pattern occurrence in the transactions within 'df_2'.\\n------\\n\")\n",
    "\n",
    "proportions = [substring_counts[pattern] / general_count for pattern in substring_patterns]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(proportions, labels=substring_patterns, autopct='%1.1f%%', colors=plt.cm.Paired.colors)\n",
    "plt.title('Task 1.8.4: Proportion of Substring Occurrences to General Pattern')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9794f7-48b2-4106-a7b3-39268da1f44f",
   "metadata": {
    "id": "2f9794f7-48b2-4106-a7b3-39268da1f44f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "znzCfu-l_5wW",
   "metadata": {
    "id": "znzCfu-l_5wW"
   },
   "source": [
    "Question 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06V_GbD_6Im",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b06V_GbD_6Im",
    "outputId": "0826d36a-f81d-4f43-8116-1d3682ba9e6b"
   },
   "outputs": [],
   "source": [
    "# Task 1.9.1: Filter the dataframe and pivot to create a transaction-level product dataframe\n",
    "\n",
    "# Filter out negative quantities and keep only the top 100 products\n",
    "top_100_products = df.groupby('productName_process')['Quantity'].sum().nlargest(100).index\n",
    "df_filtered = df[(df['productName_process'].isin(top_100_products)) & (df['Quantity'] > 0)]\n",
    "\n",
    "# Pivot the dataframe to create the transaction-level product dataframe\n",
    "transaction_df = df_filtered.pivot_table(index='TransactionNo', columns='productName_process', values='Quantity', fill_value=0)\n",
    "\n",
    "# Display the first 5 rows of the pivoted dataframe\n",
    "print(\"Transaction-Level Product DataFrame:\")\n",
    "print(transaction_df.head(5))\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the first 5 rows of the transaction-level product dataframe, where each row represents a transaction, and each column represents the quantity of a product in that transaction.\\n\")\n",
    "\n",
    "# Task 1.9.2: Apply the Apriori algorithm to find frequent itemsets with a minimum support of 1.5%\n",
    "\n",
    "# Convert the DataFrame to boolean (True/False) where a product is present or not\n",
    "transaction_df_bool = transaction_df > 0\n",
    "\n",
    "# Run the Apriori algorithm\n",
    "frequent_itemsets = apriori(transaction_df_bool, min_support=0.015, use_colnames=True)\n",
    "\n",
    "# Generate association rules with the Apriori algorithm using lift as the metric\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Display the first 5 association rules\n",
    "print(\"\\nAssociation Rules with Minimum Support of 1.5% and Lift > 1.0:\")\n",
    "print(rules.head())\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the first 5 association rules generated with a minimum support of 1.5% and lift > 1.0.\\n\")\n",
    "\n",
    "# Task 1.9.3: Apply the Apriori algorithm to identify items with support >= 1.0% and lift > 10\n",
    "\n",
    "# Run the Apriori algorithm with lower support\n",
    "frequent_itemsets_low_support = apriori(transaction_df_bool, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generate association rules with high lift\n",
    "rules_high_lift = association_rules(frequent_itemsets_low_support, metric=\"lift\", min_threshold=10)\n",
    "\n",
    "# Display the first 5 association rules\n",
    "print(\"\\nAssociation Rules with Support >= 1.0% and Lift > 10:\")\n",
    "print(rules_high_lift.head())\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the first 5 association rules generated with support >= 1.0% and lift > 10.\\n\")\n",
    "\n",
    "# Task 1.9.4: Explore three more examples with different support, confidence, and lift measurements\n",
    "\n",
    "# Example 1: High Confidence Rules\n",
    "rules_high_conf = association_rules(frequent_itemsets_low_support, metric=\"confidence\", min_threshold=0.8)\n",
    "print(\"\\nHigh Confidence Rules (Confidence >= 0.8):\")\n",
    "print(rules_high_conf.head())\n",
    "\n",
    "# Example 2: High Lift Rules\n",
    "rules_very_high_lift = association_rules(frequent_itemsets_low_support, metric=\"lift\", min_threshold=5.0)\n",
    "print(\"\\nHigh Lift Rules (Lift >= 5.0):\")\n",
    "print(rules_very_high_lift.head())\n",
    "\n",
    "# Example 3: Lower Support Rules\n",
    "rules_low_support = association_rules(frequent_itemsets_low_support, metric=\"support\", min_threshold=0.01)\n",
    "print(\"\\nLow Support Rules (Support >= 1.0%):\")\n",
    "print(rules_low_support.head())\n",
    "\n",
    "# Output explanation\n",
    "print(\"\\nThe above output shows the first 5 association rules generated with different configurations for support, confidence, and lift.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqgs5T3t_6pe",
   "metadata": {
    "id": "eqgs5T3t_6pe"
   },
   "source": [
    "Question 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h98EbTjJ_62m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h98EbTjJ_62m",
    "outputId": "494575ad-744d-4944-b91a-168d90488616"
   },
   "outputs": [],
   "source": [
    "# Task 1.10.1: Create the customer-product dataframe\n",
    "\n",
    "# Filter out transactions with negative quantities\n",
    "df_filtered = df[df['Quantity'] > 0]\n",
    "\n",
    "# Identify the top 100 products by quantity\n",
    "top_100_products = df_filtered.groupby('productName_process')['Quantity'].sum().nlargest(100).index\n",
    "\n",
    "# Filter the dataframe to keep only the top 100 products\n",
    "df_filtered = df_filtered[df_filtered['productName_process'].isin(top_100_products)]\n",
    "\n",
    "# Create the customer-product dataframe\n",
    "customer_product_df = df_filtered.pivot_table(index='CustomerNo',\n",
    "                                              columns='productName_process',\n",
    "                                              values='Quantity',\n",
    "                                              aggfunc='sum',\n",
    "                                              fill_value=0)\n",
    "\n",
    "# Display the first few rows to verify the DataFrame\n",
    "print(\"Customer-Product DataFrame:\")\n",
    "print(customer_product_df.head())\n",
    "\n",
    "# Task 1.10.2: Calculate pairwise Euclidean distance\n",
    "\n",
    "# Calculate pairwise Euclidean distance between customers\n",
    "distances = euclidean_distances(customer_product_df)\n",
    "\n",
    "# Convert the distance matrix to a DataFrame for easier interpretation\n",
    "distance_df = pd.DataFrame(distances, index=customer_product_df.index, columns=customer_product_df.index)\n",
    "\n",
    "# Display the distance DataFrame\n",
    "print(\"\\nPairwise Euclidean Distance between Customers:\")\n",
    "print(distance_df.head())\n",
    "\n",
    "# Task 1.10.3: Find the top 3 most similar customers\n",
    "\n",
    "def find_top_similar_customers(customer_no, distance_df):\n",
    "    if customer_no in distance_df.index:\n",
    "        customer_index = distance_df.index.get_loc(customer_no)\n",
    "        similar_customers = distance_df.iloc[customer_index].argsort()[1:4]  # Exclude the customer themselves\n",
    "        similar_customers_indices = distance_df.index[similar_customers]\n",
    "        return similar_customers_indices.tolist()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Find top 3 similar customers for CustomerNo 13069 and 17490\n",
    "similar_customers_13069 = find_top_similar_customers('13069', distance_df)\n",
    "similar_customers_17490 = find_top_similar_customers('17490', distance_df)\n",
    "\n",
    "print(f\"\\nTop 3 similar customers to 13069: {similar_customers_13069}\")\n",
    "print(f\"Top 3 similar customers to 17490: {similar_customers_17490}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed98589-5b20-4d85-8511-00d6ff393f8e",
   "metadata": {
    "id": "9ed98589-5b20-4d85-8511-00d6ff393f8e"
   },
   "source": [
    "## Task 1.10.4: Coding Logic for Recommending Products to CustomerNo 13069:\n",
    "\n",
    "### 1. Identify Similar Customers:\n",
    "Use the euclidean_distances() function on the customer_product_df to calculate the distances between CustomerNo 13069 and all other customers.\n",
    "Sort the resulting distances in ascending order to find the customers most similar to CustomerNo 13069.\n",
    "Select the top 3-5 customers with the smallest distance values.\n",
    "#### Calculate pairwise distances\n",
    "distances = euclidean_distances(customer_product_df)\n",
    "#### Create a distance DataFrame for easier access\n",
    "distance_df = pd.DataFrame(distances, index=customer_product_df.index, columns=customer_product_df.index)\n",
    "#### Get distances for CustomerNo 13069\n",
    "similar_customers = distance_df.loc['13069'].sort_values().index[1:4]\n",
    "\n",
    "### 2. Aggregate Purchase Data from Similar Customers:\n",
    "For the similar customers identified, aggregate their purchase data using the mean() or sum() function to calculate the average or total quantity purchased for each product.\n",
    "\n",
    "#### Aggregate the purchase data of similar customers\n",
    "similar_customers_products = customer_product_df.loc[similar_customers].mean()\n",
    "\n",
    "### 3. Filter Out Already Purchased Products:\n",
    "\n",
    "Retrieve the products already purchased by CustomerNo 13069 using .loc[].\n",
    "Compare the list of products from the similar customers with the list of products that CustomerNo 13069 has already bought.\n",
    "Use a filter to exclude products that CustomerNo 13069 has already purchased.\n",
    "#### Get products that CustomerNo 13069 has already purchased\n",
    "target_customer_products = customer_product_df.loc['13069']\n",
    "#### Filter out products already purchased by CustomerNo 13069\n",
    "recommendations = similar_customers_products[(similar_customers_products > 0) & (target_customer_products == 0)]\n",
    "### 4. Rank Potential Recommendations:\n",
    "\n",
    "Sort the filtered list of products by the aggregated quantity (using sort_values()), prioritizing those that were purchased more frequently or in larger quantities by similar customers.\n",
    "#### Rank the remaining products based on the quantity purchased by similar customers\n",
    "recommendations = recommendations.sort_values(ascending=False)\n",
    "### 5. Suggest Top Products:\n",
    "\n",
    "Present the top-ranked products as recommendations. These are the products that CustomerNo 13069 hasn't purchased yet but were popular among similar customers.\n",
    "#### Final list of recommendations for CustomerNo 13069\n",
    "print(f\"Product recommendations for CustomerNo 13069:\")\n",
    "print(recommendations.head())  # Suggest the top products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZbMZW0iv0vOZ",
   "metadata": {
    "id": "ZbMZW0iv0vOZ"
   },
   "source": [
    "# Part II Sales Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oIgcp6Ufd4es",
   "metadata": {
    "id": "oIgcp6Ufd4es"
   },
   "source": [
    "### Question 2.1: Time Series Analysis and Filling Missing Dates\n",
    "\n",
    "In this task, we analyze the revenue time series by handling missing dates and performing seasonal decomposition to identify trends, seasonal patterns, and residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gDrlP3xrx-SG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gDrlP3xrx-SG",
    "outputId": "669627cb-d995-4dba-97d9-b9256ad07a22"
   },
   "outputs": [],
   "source": [
    "# Step 1: Creating a 'revenue' column by multiplying 'Price' and 'Quantity'\n",
    "\n",
    "df['revenue'] = df['Price'] * df['Quantity']\n",
    "\n",
    "# Step 2: Converting the 'Date' column to a datetime format\n",
    "\n",
    "df['transaction_date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Step 3: Grouping by 'transaction_date' to calculate daily revenue\n",
    "\n",
    "df_daily_revenue = df.groupby('transaction_date').agg({'revenue': 'sum'}).reset_index()\n",
    "\n",
    "# Step 4: Setting 'transaction_date' as the index for time series analysis\n",
    "\n",
    "df_daily_revenue.set_index('transaction_date', inplace=True)\n",
    "\n",
    "# Step 5: Filling in missing dates by resampling to daily frequency\n",
    "\n",
    "mean_revenue = df_daily_revenue['revenue'].mean()  # Calculate mean revenue\n",
    "df_daily_revenue_filled = df_daily_revenue.resample('D').sum().fillna(mean_revenue)  # Fill missing dates with mean\n",
    "\n",
    "# Step 6: Decomposing the time series using an additive model\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(df_daily_revenue_filled['revenue'], model='additive')\n",
    "\n",
    "# Step 7: Plotting the decomposition (Observed, Trend, Seasonal, Residuals)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10))\n",
    "decomposition.observed.plot(ax=ax1, title='Observed')\n",
    "decomposition.trend.plot(ax=ax2, title='Trend')\n",
    "decomposition.seasonal.plot(ax=ax3, title='Seasonal')\n",
    "decomposition.resid.plot(ax=ax4, title='Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Displaying the first few rows of the filled daily revenue data\n",
    "\n",
    "df_daily_revenue_filled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08BZcvJ9fyxE",
   "metadata": {
    "id": "08BZcvJ9fyxE"
   },
   "source": [
    "1.   **Step 1:** This will give us the total revenue for each transaction.\n",
    "2.   **Step 2:** This is crucial for time-series analysis and ensures that the dates are in the correct format.\n",
    "3.   **Step 3:** We want to aggregate the total revenue for each day, not for each transaction.\n",
    "4.   **Step 5:** Here, we fill in any missing dates and assign the mean revenue to these missing dates.\n",
    "5.   **Step 6:** Decomposing the time series into its components: observed, trend, seasonal, and residual.\n",
    "6.   **Step 7:** This will help us understand the different components of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MIjCAPCJkxK9",
   "metadata": {
    "id": "MIjCAPCJkxK9"
   },
   "source": [
    "### Analysis of Results\n",
    "\n",
    "- **Observed**: Displays the overall revenue trends over time.\n",
    "- **Trend**: Shows long-term increases or decreases in revenue.\n",
    "- **Seasonal**: Identifies any recurring patterns, such as weekly or monthly cycles.\n",
    "- **Residuals**: Represents the randomness or irregularities after removing the trend and seasonal components.\n",
    "\n",
    "The missing dates were filled with the mean revenue to ensure a continuous time series for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ueAMfTXO0-aj",
   "metadata": {
    "id": "ueAMfTXO0-aj"
   },
   "source": [
    "## Question 2.2: ARIMA Time Series Model\n",
    "\n",
    "We will use the ARIMA model to forecast future revenue and find the best model by testing different parameters `(p, d, q)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cGcxXFm6pg6z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "cGcxXFm6pg6z",
    "outputId": "49d591a2-2ccf-481f-e2ae-b893ccef108f"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensuring we have the df_daily_revenue_filled data from Question 2.1\n",
    "\n",
    "# Step 1: Splitting the data into training and test sets\n",
    "\n",
    "train = df_daily_revenue_filled.loc[:'2019-11-01']\n",
    "test = df_daily_revenue_filled.loc['2019-11-02':]\n",
    "\n",
    "# Step 2: Defining the range for p, d, q values for grid search\n",
    "\n",
    "p = d = q = range(0, 3)\n",
    "pdq = [(x, y, z) for x in p for y in d for z in q]\n",
    "\n",
    "# Step 3: Initializing variables to track the best parameters and the lowest error\n",
    "\n",
    "best_pdq = None\n",
    "best_mae = float(\"inf\")\n",
    "\n",
    "# Step 4: Performing grid search over ARIMA parameters\n",
    "\n",
    "for param in pdq:\n",
    "    try:\n",
    "        # Fitting ARIMA model\n",
    "\n",
    "        model = ARIMA(train['revenue'], order=param)\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Forecasting on the test set\n",
    "\n",
    "        forecast = model_fit.forecast(steps=len(test))\n",
    "\n",
    "        # Calculating MAE\n",
    "\n",
    "        mae = mean_absolute_error(test['revenue'], forecast)\n",
    "\n",
    "        # Updating the best model if the current one is better\n",
    "\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_pdq = param\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Step 5: Outputting the best parameters and the corresponding MAE\n",
    "\n",
    "print(f'Best ARIMA parameters: {best_pdq}')\n",
    "print(f'Best Mean Absolute Error (MAE): {best_mae}')\n",
    "\n",
    "# Step 6: Fitting the ARIMA model with the best parameters and plotting the forecast vs actual\n",
    "\n",
    "best_model = ARIMA(train['revenue'], order=best_pdq)\n",
    "best_model_fit = best_model.fit()\n",
    "\n",
    "# Forecasting on test set\n",
    "\n",
    "forecast = best_model_fit.forecast(steps=len(test))\n",
    "\n",
    "# Plotting the forecasted values against the actual test set\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test.index, test['revenue'], label='Actual')\n",
    "plt.plot(test.index, forecast, label='Forecast', color='red')\n",
    "plt.title('ARIMA Forecast vs Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Revenue')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jZpTtUE1AMz",
   "metadata": {
    "id": "9jZpTtUE1AMz"
   },
   "source": [
    "### ***Question 2.3***\n",
    "\n",
    "In this question, we examine many deep learning techniques for time series forecasting and describe the required procedures for modelling and data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5O8a0M7tDGf",
   "metadata": {
    "id": "g5O8a0M7tDGf"
   },
   "source": [
    "**Explored Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5OOTm0OtRHg",
   "metadata": {
    "id": "A5OOTm0OtRHg"
   },
   "source": [
    "**1. LSTM (Long Short-Term Memory Networks):**\n",
    "\n",
    "\n",
    "\n",
    "*   LSTM, a variant of recurrent neural networks (RNN), can learn long-term dependencies in sequence data.\n",
    "*   Designed to remember trends over extended periods, LSTMs are extensively used for time series forecasting and fit financial, weather, and other sequential information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zRYjerk8vL97",
   "metadata": {
    "id": "zRYjerk8vL97"
   },
   "source": [
    "**2. GRU (Gated Recurrent Unit):**\n",
    "\n",
    "\n",
    "\n",
    "*   Reducing the number of gates (mechanisms for information flow) helps GRU, a variation of LSTM, to simplify the model.\n",
    "*   On many time series forecasting projects, GRUs perform comparably to LSTMs and are computationally more economical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KpQNX4I7yCF7",
   "metadata": {
    "id": "KpQNX4I7yCF7"
   },
   "source": [
    "**3. TCN (Temporal Convolutional Networks):**\n",
    "\n",
    "\n",
    "\n",
    "*   Temporal Convolutional Network (TCN) is a convolutional neural network structure specifically developed for sequential data.\n",
    "*   Because TCNs employ causal convolutions rather than LSTMs and GRUs, predictions made at a particular time step are solely impacted by information from the past.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ihdgEklZ0Cw4",
   "metadata": {
    "id": "ihdgEklZ0Cw4"
   },
   "source": [
    "**Steps for Data Wrangling and Modeling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch40Q2261Nja",
   "metadata": {
    "id": "ch40Q2261Nja"
   },
   "source": [
    "**1. Data Preparation:**\n",
    "\n",
    "\n",
    "*   **Normalize or Standardize the Data:** Adjust the time series data to fall into a particular range, say between 0 and 1. This makes sure that features with wider numerical ranges are not given more weight by the neural network than others.\n",
    "\n",
    "  * Utilise programs from libraries like sklearn, such as MinMaxScaler and StandardScaler.\n",
    "\n",
    "*   **Create Time Windows or Sequences:**\n",
    "\n",
    "  *   To forecast time series, the data must be organised into sequences (also known as sliding windows) with a set duration.\n",
    "  *   Each input to the model would have 60 time steps (features), for instance, if the window size was 60. The target value would be the subsequent step or the 61st time step.\n",
    "\n",
    "*   **Split the Data into Training and Test Sets:**\n",
    "\n",
    "  *   Typically, training uses 80% of the data while testing uses 20%.\n",
    "  *   To avoid data leaking, time series data must be divided chronologically rather than arbitrarily (i.e., the model learning from future data).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_rOdBk4G4SWl",
   "metadata": {
    "id": "_rOdBk4G4SWl"
   },
   "source": [
    "**2. Model Definition:**\n",
    "\n",
    "* **LSTM/GRU Model:**\n",
    "\n",
    "  1.   **Input Shape:** It is necessary to transform the input data into 3D arrays (samples, time steps, features).\n",
    "  2.   **LSTM Layers:** Describe a layer or layers of LSTM. It is possible to change the quantity of LSTM units (neurones) in every layer. Units of 50, 100, or 200 are typical possibilities.\n",
    "  3.   **Dense Layers:** To forecast the output for the upcoming time step, a dense layer is inserted after the LSTM layers.\n",
    "  4.   **Dropout Layer:** To stop overfitting, dropout layers can be included, which randomly ignore a portion of the neurones during training.\n",
    "\n",
    "* **Temporal Convolutional Networks (TCN):**\n",
    "\n",
    "  1.   **Casual Convolutional Layers:** To make sure that previous data is the only factor influencing predictions, define convolutional layers using **causal convolutions**.\n",
    "  2.   **Dilated Convolutions:** To exponentially expand the receptive field—that is, the volume of historical data the model takes into account—use dilated convolutions.\n",
    "  3.   **Residual Connections:** To stabilise and enhance the model's capacity to manage long-term dependencies, add residual connections.\n",
    "\n",
    "* **Compile the Model:** For regression problems, use a suitable loss function such as **Mean Squared Error (MSE)**, and select an optimiser such as **Adam** to minimise the loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1Q-iMn_UCCvc",
   "metadata": {
    "id": "1Q-iMn_UCCvc"
   },
   "source": [
    "**3. Model Training:**\n",
    "\n",
    "  *   **Train the Model:** Use the training set to train the model for a predetermined number of epochs (e.g., 20-100 epochs). To enhance the performance of the model, one can adjust the batch size and number of epochs.\n",
    "  *   **Early Stopping:** Throughout training, keep an eye on validation loss and halt the model if it begins to overfit (validation loss increases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QB-p8lbOC1xu",
   "metadata": {
    "id": "QB-p8lbOC1xu"
   },
   "source": [
    "**4. Evaluate the Model:**\n",
    "\n",
    "  *   **Evaluate on the Test Set:** Utilise metrics such as **Mean Absolute Error (MAE)** or **Mean Squared Error (MSE)** to assess the model's performance after training.\n",
    "  *   **Make Predictions:** Future time steps can be predicted by using the trained model. Utilising the scaler's inverse transform, return the predictions to their initial scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ydFm5PwmDeaZ",
   "metadata": {
    "id": "ydFm5PwmDeaZ"
   },
   "source": [
    "**5. Visualize the Results:**\n",
    "\n",
    "  *   Plotting real vs. projected numbers will allow to visually evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FQic9AqID7Nc",
   "metadata": {
    "id": "FQic9AqID7Nc"
   },
   "source": [
    "**References:**\n",
    "\n",
    "**1. LSTM and GRU Networks:**\n",
    "\n",
    "*   **Hochreiter, S., & Schmidhuber, J. (1997).** \"Long Short-Term Memory\". Neural Computation, 9(8), 1735-1780. LSTM Paper\n",
    "*   **Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014).** \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". arXiv preprint arXiv:1412.3555. GRU Paper\n",
    "\n",
    "**2. Temporal Convolutional Networks (TCN):**\n",
    "\n",
    "*   **Bai, S., Kolter, J. Z., & Koltun, V. (2018).** \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\". arXiv preprint arXiv:1803.01271. TCN Paper"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
